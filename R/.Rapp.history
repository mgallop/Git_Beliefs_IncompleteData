plot(postbelief,data$v1)
plot(postbelief,data$t1)
plot(postbelief,data$v1)
table(data$t1,v1change)
table(data$v1,v1change)
.5*.3
### BELIEFS DATA QUALITY#
### Some experimentation#
### Simon, 05/19/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X1old <- X[,1]#
makebin <- ifelse(X[,1]>quantile(X[,1],0.4),1,0)#
X[,1] <- makebin#
X <- cbind(rnorm(n,1,1),X)#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: x1=0 if t1=0, if t1=1 then x1=1 w/ prob pi_i#
#
# model: logit(pi_1) = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- X1old + rnorm(n,0,0)#
pihelp <- a0 + a1*w1#
pi <- exp(pihelp)/(1+exp(pihelp))#
#
#
replace <- rbinom(n,1,pi)#
x1 <- ifelse(data$t1==1,replace,0)#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0)#
#
pimatbelief <- NULL#
for(i in 1:m){#
	pihelpbelief <- a0belief + a1belief[i]*w1#
	pibelief <- exp(pihelpbelief)/(1+exp(pihelpbelief))#
	pimatbelief <- cbind(pimatbelief,pibelief)#
}
newpost <- NULL#
#
for(i in 1:m){#
	relpibelief <- pimatbelief[,i]#
	# P(t=1 | x=0)#
	gamma <- mean(data$v1)/mean(relpibelief)#
	postbelief <- ((1-relpibelief)*gamma)/(1-relpibelief*gamma)#
	replace <- rbinom(n,1,postbelief)#
	v1change <- ifelse(data$v1==1,1,replace)#
	#v1change <- ifelse(data$v1==1,1,postbelief)#
	data$v1change <- v1change#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}
gamma
mean(data$t1)
mean(data$v1)
#
newpost <- as.data.frame(newpost)#
mean(newpost$v1change)#
#
plot(density(mtrue[,2]),xlim=c(0,3))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
newpost <- NULL#
#
for(i in 1:m){#
	relpibelief <- pimatbelief[,i]#
	# P(t=1 | x=0)#
	#gamma <- mean(data$v1)/mean(relpibelief)#
	gamma <- 0.6#
	postbelief <- ((1-relpibelief)*gamma)/(1-relpibelief*gamma)#
	replace <- rbinom(n,1,postbelief)#
	v1change <- ifelse(data$v1==1,1,replace)#
	#v1change <- ifelse(data$v1==1,1,postbelief)#
	data$v1change <- v1change#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
mean(newpost$v1change)#
#
plot(density(mtrue[,2]),xlim=c(0,3))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- t1 + m1#
#
#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)
head(X)
### BELIEFS DATA QUALITY#
### Some experimentation#
### Simon, 05/19/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X1old <- X[,1]#
makebin <- ifelse(X[,1]>quantile(X[,1],0.4),1,0)#
X[,1] <- makebin
head(X)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- t1 + m1#
#
#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1
cor(data$t1,data$v1)
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0)#
#
pimatbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	pimatbelief <- cbind(pimatbelief,pibelief)#
}
a0belief <- 0#
a1belief <- rnorm(m,1,0)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}
cor(m1belief,m1)
i<-1
newpost <- NULL
	relm1belief <- m1matbelief[,i]
relm1belief
	v1change <- data$v1 - m1
cor(v1change,data$v1)
cor(v1change,data$t1)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1change <- data$v1 - m1#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
mean(newpost$v1change)#
#
plot(density(mtrue[,2]),xlim=c(0,3))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
m <- 100
a1belief <- rnorm(m,1,0.5)
plot(density(a1belief))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1change <- data$v1 - m1#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
mean(newpost$v1change)#
#
plot(density(mtrue[,2]),xlim=c(0,3))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
head(newpost)
summary(mobs)
colnames(newpost)
colnames(newpost)[1]
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3)
output
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)
output
summary(mtrue)
plot(density(mtrue[,2]),xlim=c(0,3))
lines(density(mobs[,2]),type="l",col="blue")
lines(density(newpost$v1change),type="l",col="red")
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1change <- data$v1 - m1#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
#
plot(density(mtrue[,2]),xlim=c(0,4))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,4))
lines(density(mobs[,2]),type="l",col="blue")
lines(density(newpost$v1change),type="l",col="red")
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,5))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,10))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9))#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")
plot(density(mtrue[,2]),xlim=c(0,3),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(w1,m1belief)
plot(w1,m1matbelief)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1change <- data$v1 - m1#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
quartz("",15,5)#
layout(cbind(1,2,3))#
layout.show(3)
head(newpost)
plot(density(mtrue[,3]),xlim=c(0,3),ylim=c(0,9),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(mtrue[,4]),xlim=c(0,3),ylim=c(0,9),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(mtrue[,4]),xlim=c(-1,2),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(mtrue[,3]),xlim=c(0,3),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(mtrue[,4]),xlim=c(-1,2),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
quartz("",5,15)#
layout(rbind(1,2,3))#
layout.show(3)
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,2),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)
plot(density(data$t1))
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)
head(X)
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)
head(X)
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)
head(X)
summary(X[,1])
## make the first one into count#
for(i in 1:10){#
	X[,1] <- ifelse[X[,1]<0,rnorm(1,4,1),X[,1]]#
	}
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}
head(X)
bla <- rpois(1,X[,1])
bla
dim(X)
help <- rep(NA,dim(X)[1])
help
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}
help
X[,1] <- help
cor(X[,1],X[,2])
cor(X[,1],X[,3])
cor(X[,1],X[,4])
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we subtract a Poisson#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# log link#
w1 <- rnorm(n,0,1)#
m1 <- exp(a0 + a1*w1)
m1
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we subtract a Poisson#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
log(rate1) <- a0 + a1*w1
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we subtract a Poisson#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- log(a0 + a1*w1)#
m1 <- rpois(n,rate1)
log(a0 + a1*w1)
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we subtract a Poisson#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)
m1
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we subtract a Poisson#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)#
#
x1 <- data$t1 - m1#
data$v1 <- x1
x1
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we subtract a Poisson#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- .1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)#
#
x1 <- data$t1 - m1#
data$v1 <- x1
x1
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
## we treat X[,2] as x1, now we get t1=x1+m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.5#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)#
#
t1 <- X[,2] + m1#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*t1 + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,t1,X[,3],X[,4],X[,2])#
colnames(data) <- c("dv","t1","v2","v3","v1")#
data <- as.data.frame(data)
head(data)
data$t1-data$v1
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
## we treat X[,2] as x1, now we get t1=x1+m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.5#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)#
#
t1 <- X[,2] + m1#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*t1 + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,t1,X[,3],X[,4],X[,2])#
colnames(data) <- c("dv","t1","v2","v3","x1")#
data <- as.data.frame(data)#
#
#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
## we treat X[,2] as x1, now we get t1=x1+m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)#
#
t1 <- X[,2] + m1#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*t1 + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,t1,X[,3],X[,4],X[,2])#
colnames(data) <- c("dv","t1","v2","v3","x1")#
data <- as.data.frame(data)#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
## we treat X[,2] as x1, now we get t1=x1+m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)#
#
t1 <- X[,2] + m1#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*t1 + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,t1,X[,3],X[,4],X[,2])#
colnames(data) <- c("dv","t1","v2","v3","x1")#
data <- as.data.frame(data)#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	rate1belief <- exp(a0belief + a1belief[i]*w1)#
	m1belief <- rpois(n,rate1belief)#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}
cor(m1belief,m1)
i<-1
	relm1belief <- m1matbelief[,i]
relm1belief
	v1change <- data$v1 - m1
v1change
data$v1
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + x1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}
i<-1
	relm1belief <- m1matbelief[,i]
	v1change <- data$v1 - m1
v1change
data$v1
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)
### BELIEFS DATA QUALITY#
### Poisson#
### Simon, 05/25/12#
#
### In this example we take the true data t1 to be a Poisson count and subtract another count (here Poisson, can also be binomial, geometric, neg binomial) variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable count is v1=t1-m1 (can also be t1+m1 if we expect that the reported counts are larger than the real one's). I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1+m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(4,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
#
## make the first one into count: use the normal draw as the rate parameter#
for(i in 1:10){#
	X[,1] <- ifelse(X[,1]<0,rnorm(1,4,1),X[,1])#
	}#
#
help <- rep(NA,dim(X)[1])#
for(i in 1:dim(X)[1]){#
	help[i] <- rpois(1,X[i,1])#
	}#
#
X[,1] <- help#
#
# add intercept	#
X <- cbind(rnorm(n,1,1),X)#
#
#
## we treat X[,2] as x1, now we get t1=x1+m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
# rate paramter modeled with log link#
w1 <- rnorm(n,0,1)#
rate1 <- exp(a0 + a1*w1)#
m1 <- rpois(n,rate1)#
#
t1 <- X[,2] + m1#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*t1 + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,t1,X[,3],X[,4],X[,2])#
colnames(data) <- c("dv","t1","v2","v3","v1")#
data <- as.data.frame(data)#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	rate1belief <- exp(a0belief + a1belief[i]*w1)#
	m1belief <- rpois(n,rate1belief)#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}
i<-1
	relm1belief <- m1matbelief[,i]
	v1change <- data$v1 - m1
v1change
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1change <- data$v1 - relm1belief#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
relm1belief
cor(m1, relm1belief)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1change <- data$v1 - relm1belief#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1change <- data$v1 - relm1belief#
	m2 <- MCMCregress(data$dv ~ 1 + v1change + data$v2 + data$v3, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
newpost$v2
newpost
head(newpost)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.5)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(relm1belief,m1)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.01)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
a0belief <- 0#
a1belief <- rnorm(m,1,0.2)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(density(a1belief))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- 1#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=10000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- 1#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=10000, thin=25)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 1#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- 1.1#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=10000, thin=25)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,1,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=10000, thin=25)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
plot(w1,m1belief)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,2,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
plot(w1,m1belief)
head(m1belief)
head(m1beliefmat)
head(m1matbelief)
dim(m1matbelief)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 100#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,2,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}
dim(m1matbelief)
head(m1matbelief)
attach(geyser)#
plot(duration, waiting, xlim = c(0.5,6), ylim = c(40,100))#
f1 <- kde2d(duration, waiting, n = 50, lims = c(0.5, 6, 40, 100))#
image(f1, zlim = c(0, 0.05))
plot(duration, waiting, xlim = c(0.5,6), ylim = c(40,100))
i<-1
	quantile(m1matbelief[i,],c(0.025,0.5,0.975))
w1
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))
borders
borders[1]
borders[]
borders[[]]
borders[1:3]
borders[1:3,]
borders[,1:3]
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- cbind(bordermat,borders)#
}
bordermat
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}
bordermat
bordermat[,2]
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 100#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,2,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(w1,bordermat[,2],type="l")#
points(distvec,bordermat[,1],type="l",lty=2)#
points(distvec,bordermat[,3],type="l",lty=2)
plot(w1,bordermat[,2],type="l")#
points(w1,bordermat[,1],type="l",lty=2)#
points(w1,bordermat[,3],type="l",lty=2)
bordermat[,1]
plot(w1,bordermat[,1],type="l",lty=2)
plot(w1,bordermat[,1])
plot(w1,bordermat[,2])
plot(w1,bordermat[,3])
plot(w1,bordermat[,3],type="l")
sort(w1)
plot(sort(w1),sort(bordermat[,2]),type="l")
plot(sort(w1),sort(bordermat[,2]),type="l")#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
plot(sort(w1),sort(bordermat[,2]),type="l",ylim=c(min(bordermat[,1]),max(bordermat[,3])))
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
identify()
plot(sort(w1),sort(bordermat[,2]),type="p",lty=3,ylim=c(min(bordermat[,1]),max(bordermat[,3])))
plot(sort(w1),sort(bordermat[,2]),type="p",pty=3,ylim=c(min(bordermat[,1]),max(bordermat[,3])))
plot(sort(w1),sort(bordermat[,2]),type="p",pty=1,ylim=c(min(bordermat[,1]),max(bordermat[,3])))
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,2,0.2)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 100#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,2,0.2)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=10000, thin=25)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 100#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)
plot(density(mtrue[,2]))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 100#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,2,0.2)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 100#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,1,0.2)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 100#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,1,0.2)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Experimentation with Normal#
### Simon, 05/24/12#
#
### In this example we take the true data t1 to be normally distributed and add another normally distributed variable m1 <- a0 + a1*w1, where w1 is the variable that determines the "false reporting". The observed variable is v1=t1+m1. I then assume that we know the true a0 (with certainty) and a1 (correct mean but some uncertainty). From this a distribution of m1 (m1hat) is constructed and then the "expected" t1 is computed: t1hat=v1-m1hat. At the end we see that the coefficient b1 we care about is biased when v1 is taken at face value but not when we correct using the correct beliefs.#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
# t1 is binomial and is the true values#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: we add another normal m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1 + m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,1,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
#
for(i in 1:m){#
	relm1belief <- m1matbelief[,i]#
	v1changevec <- data$v1 - relm1belief#
	data$v1change <- v1changevec#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.1#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1
data$v1 <- x1
plot(data$v1,data$t1)
seq(min(data$v1),max(data$v1),0.1)
points(seq(min(data$v1),max(data$v1),0.1),seq(min(data$v1),max(data$v1),0.1),type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.4#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
plot(data$v1,data$t1)#
points(seq(min(data$v1),max(data$v1),0.1),seq(min(data$v1),max(data$v1),0.1),type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.3#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
plot(data$v1,data$t1)#
points(seq(min(data$v1),max(data$v1),0.1),seq(min(data$v1),max(data$v1),0.1),type="l",col="red")
plot(data$v1,data$t1,xlim=c(-10,10))
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.3#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
plot(data$v1,data$t1,xlim=c(-5,10))#
points(seq(-5,10,0.1),seq(-5,10,0.1),type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 1#
a1belief <- rnorm(m,0.2,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 500#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.2#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.2,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.15#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
# pi_i = probability x1=1 if t1=1#
# model: logit(pi_1) = a0 + a1 w1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.15,0.1)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
i<-1
	m1belief <- a0belief + a1belief[i]*w1
m1belief
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.15#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.15,0.1)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.15#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.15,0.03)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.15#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 50#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.15,0.03)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
for(i in 1:m){#
	data$v1change <- t1matbelief[,i]#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
summary(mtrue)
output
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.15#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.15,0.03)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
for(i in 1:m){#
	data$v1change <- t1matbelief[,i]#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.15#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.15,0.01)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
for(i in 1:m){#
	data$v1change <- t1matbelief[,i]#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0.05#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0.05,0.01)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
#
#
#
### CHANGING X1 ACCORDING TO BELIEFS#
#
newpost <- NULL#
for(i in 1:m){#
	data$v1change <- t1matbelief[,i]#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
points(seq(min(w1),max(w1)),seq(min(w1),max(w1)),0.01,type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.05#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 1#
a1belief <- rnorm(m,0.05,0.005)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
points(seq(min(w1),max(w1)),seq(min(w1),max(w1)),0.01,type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- -1#
a1 <- 0.05#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- -1#
a1belief <- rnorm(m,0.05,0.005)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
points(seq(min(w1),max(w1)),seq(min(w1),max(w1)),0.01,type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 1#
a1belief <- rnorm(m,0,0.005)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
points(seq(min(w1),max(w1)),seq(min(w1),max(w1)),0.01,type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 0#
a1 <- 0#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 0#
a1belief <- rnorm(m,0,0.005)#
#
t1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	t1belief <- m1belief*data$v1#
	t1matbelief <- cbind(t1matbelief,t1belief)#
}#
#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(t1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
points(seq(min(w1),max(w1)),seq(min(w1),max(w1)),0.01,type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.02#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 1#
a1belief <- rnorm(m,0.02,0.005)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)#
points(seq(min(w1),max(w1)),seq(min(w1),max(w1)),0.01,type="l",col="red")
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.02#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1#
a1 <- 0.05#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 1#
a1belief <- rnorm(m,0.05,0.01)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
#
#
newpost <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	data$v1change <- m1belief*data$v1#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
### BELIEFS DATA QUALITY#
### Simon, 05/26/12#
#
#
rm(list=ls(all=TRUE))#
#
library(MASS)#
library(MCMCpack)#
#
### CREATE DATA SET#
# model: y= b0 + b1 t1 + b2 x2 + b3 x3 + e#
#
n <- 1000#
#
# Design matrix X#
#
#correlations between the variables#
corr12 <- 0.3#
corr13 <- 0.8#
corr23 <- -0.2#
#
mu <- c(1,-4,3)#
Sigma <- matrix(c(1,corr12,corr13,corr12,1,corr23,corr13,corr23,1),3,3)#
#
X <- mvrnorm(n, mu, Sigma)#
X <- cbind(rnorm(n,1,1),X)#
#
#
# Dependent Variable Y#
#
# the regression coefficients#
b1 <- 2#
b2 <- 0.4#
b3 <- -0.2#
#
Y <- X[,1] + b1*X[,2] + b2*X[,3] + b3*X[,4] + rnorm(n,0,1)#
#
data <- cbind(Y,X[,2],X[,3],X[,4])#
colnames(data) <- c("dv","t1","v2","v3")#
data <- as.data.frame(data)#
#
#
## x1 is realization of t1: t1=x1*m1, so x1=t1/m1#
#
# model: m1 = a0 + a1 w1#
a0 <- 1.2#
a1 <- 0.05#
#
w1 <- rnorm(n,0,1)#
m1 <- a0 + a1*w1#
#
x1 <- data$t1/m1#
data$v1 <- x1#
#
#
mtrue <- MCMCregress(dv ~ 1 + t1 + v2 + v3, data=data)#
summary(mtrue)#
#
mobs <- MCMCregress(dv ~ 1 + v1 + v2 + v3, data=data)#
summary(mobs)#
#
#
### QUANTIFY BELIEFS ABOUT X1#
#
# length of the vector of beliefs on a0, a1#
m <- 100#
#
# here we get it right for a0 and have the right mean for a1#
	# the variance needs to be pretty low#
a0belief <- 1.2#
a1belief <- rnorm(m,0.05,0.01)#
#
m1matbelief <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	m1matbelief <- cbind(m1matbelief,m1belief)#
}#
#
bordermat <- NULL#
for(i in 1:n){#
	borders <- quantile(m1matbelief[i,],c(0.025,0.5,0.975))#
	bordermat <- rbind(bordermat,borders)#
}#
#
#
plot(sort(w1),sort(bordermat[,2]),type="p",ylim=c(min(bordermat[,1]),max(bordermat[,3])))#
points(sort(w1),sort(bordermat[,1]),type="l",lty=2)#
points(sort(w1),sort(bordermat[,3]),type="l",lty=2)
#
newpost <- NULL#
for(i in 1:m){#
	m1belief <- a0belief + a1belief[i]*w1#
	data$v1change <- m1belief*data$v1#
	m2 <- MCMCregress(dv ~ 1 + v1change + v2 + v3, data=data, burnin=1000, mcmc=5000, thin=50)#
	newpost <- rbind(newpost,m2)#
}#
#
newpost <- as.data.frame(newpost)#
#
# regression output#
output <- matrix(c("", "Mean", "SD", colnames(newpost)[1], mean(newpost[,1]), sqrt(var(newpost[,1])), colnames(newpost)[2], mean(newpost[,2]), sqrt(var(newpost[,2])), colnames(newpost)[3], mean(newpost[,3]), sqrt(var(newpost[,3])), colnames(newpost)[4], mean(newpost[,4]), sqrt(var(newpost[,4]))),ncol=3,byrow=T)#
output#
#
# plotting and comparing 3 coefficients#
quartz("",5,8)#
layout(rbind(1,2,3))#
layout.show(3)#
#
plot(density(mtrue[,2]),xlim=c(0,3),ylim=c(0,9),main="Posterior of Coefficient of Interest")#
lines(density(mobs[,2]),type="l",col="blue")#
lines(density(newpost$v1change),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,3]),xlim=c(0,2),ylim=c(0,8),main="Posterior of V2")#
lines(density(mobs[,3]),type="l",col="blue")#
lines(density(newpost$v2),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))#
#
plot(density(mtrue[,4]),xlim=c(-1,3),ylim=c(0,7),main="Posterior of V3")#
lines(density(mobs[,4]),type="l",col="blue")#
lines(density(newpost$v3),type="l",col="red")#
legend("topright",c("True","Observed","Correct Belief"),col=c("black","blue","red"),lty=c(1,1,1))
